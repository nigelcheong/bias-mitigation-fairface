{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e448e3",
   "metadata": {},
   "source": [
    "# Feature Engineering w/ Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6091735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b77a8",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738ff794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip files\n",
    "zip_025 = '../data/fairface-img-margin025-trainval.zip'\n",
    "zip_125 = '../data/fairface-img-margin125-trainval.zip'\n",
    "\n",
    "# Extraction directories\n",
    "extract_dir_025 = '../data/fairface_025'\n",
    "extract_dir_125 = '../data/fairface_125'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1693f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\fairface_025 already exists, skipping extraction.\n",
      "..\\data\\fairface_125 already exists, skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "def extract_zip(zip_path, extract_to):\n",
    "    zip_path = Path(zip_path)\n",
    "    extract_to = Path(extract_to)\n",
    "    \n",
    "    if not extract_to.exists():\n",
    "        print(f\"Extracting {zip_path.name}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Extracted to {extract_to}\")\n",
    "    else:\n",
    "        print(f\"{extract_to} already exists, skipping extraction.\")\n",
    "\n",
    "# Extract both datasets\n",
    "extract_zip(zip_025, extract_dir_025)\n",
    "extract_zip(zip_125, extract_dir_125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02308e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation labels\n",
    "train_labels = pd.read_csv('../data/fairface_label_train.csv')\n",
    "val_labels = pd.read_csv('../data/fairface_label_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a0823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 97698\n",
      "+-----------+-----+------+----------+------------+\n",
      "|       file|  age|gender|      race|service_test|\n",
      "+-----------+-----+------+----------+------------+\n",
      "|train/1.jpg|50-59|  Male|East Asian|        true|\n",
      "|train/2.jpg|30-39|Female|    Indian|       false|\n",
      "|train/3.jpg|  3-9|Female|     Black|       false|\n",
      "|train/4.jpg|20-29|Female|    Indian|        true|\n",
      "|train/5.jpg|20-29|Female|    Indian|        true|\n",
      "+-----------+-----+------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine train and validation labels using Apache Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "\t.appName(\"FacialRecognitionBiasMitigation\") \\\n",
    "\t.config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "\t.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "\t.getOrCreate()\n",
    "\n",
    "train_df = spark.createDataFrame(train_labels)\n",
    "val_df = spark.createDataFrame(val_labels)\n",
    "combined_df = train_df.union(val_df)\n",
    "combined_df.cache()\n",
    "print(f\"Total records: {combined_df.count()}\")\n",
    "combined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85618210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- service_test: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show data types\n",
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825e8084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+----+------------+\n",
      "|file|age|gender|race|service_test|\n",
      "+----+---+------+----+------------+\n",
      "|   0|  0|     0|   0|           0|\n",
      "+----+---+------+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "missing_counts = combined_df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined_df.columns])\n",
    "missing_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b120af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'age': ['30-39', '20-29', '60-69', '3-9', 'more than 70', '10-19', '40-49', '0-2', '50-59']\n",
      "Unique values in 'gender': ['Female', 'Male']\n",
      "Unique values in 'race': ['Indian', 'Latino_Hispanic', 'Southeast Asian', 'White', 'Middle Eastern', 'Black', 'East Asian']\n"
     ]
    }
   ],
   "source": [
    "# Show unique values in categorical columns\n",
    "categorical_columns = ['age', 'gender', 'race']\n",
    "for col in categorical_columns:\n",
    "    unique_values = combined_df.select(col).distinct().collect()\n",
    "    unique_values_list = [row[col] for row in unique_values]\n",
    "    print(f\"Unique values in '{col}': {unique_values_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1a637",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000795b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute brightness as a UDF\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def compute_brightness(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('L')  # Convert to grayscale\n",
    "            stat = Image.Stat.Stat(img)\n",
    "            return float(stat.mean[0])\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "brightness_udf = udf(compute_brightness, FloatType())\n",
    "\n",
    "# Compute contrast as a UDF\n",
    "def compute_contrast(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('L')  # Convert to grayscale\n",
    "            stat = Image.Stat.Stat(img)\n",
    "            return float(stat.stddev[0])\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "contrast_udf = udf(compute_contrast, FloatType())\n",
    "\n",
    "# Compute sharpness as a UDF\n",
    "def compute_sharpness(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('L')  # Convert to grayscale\n",
    "            stat = Image.Stat.Stat(img)\n",
    "            return float(stat.extrema[1] - stat.extrema[0])\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "sharpness_udf = udf(compute_sharpness, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "769516e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UDFs to compute brightness and contrast\n",
    "combined_df = combined_df.withColumn('brightness', brightness_udf(col('file')))\n",
    "combined_df = combined_df.withColumn('contrast', contrast_udf(col('file')))\n",
    "combined_df = combined_df.withColumn('sharpness', sharpness_udf(col('file')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8139aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- service_test: boolean (nullable = true)\n",
      " |-- brightness: float (nullable = true)\n",
      " |-- brightness: float (nullable = true)\n",
      " |-- contrast: float (nullable = true)\n",
      " |-- sharpness: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show schema after adding new features\n",
    "combined_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
