{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d17975-2b42-4fc0-a910-e6266e5d71f0",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40fa86c-4494-40e2-994d-e87add5affc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf1a0c-98c5-45ea-aff9-053adac03752",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850179ca-1f8c-4c92-b409-03c09527ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip files\n",
    "zip_025 = '../data/fairface-img-margin025-trainval.zip'\n",
    "zip_125 = '../data/fairface-img-margin125-trainval.zip'\n",
    "\n",
    "# Extraction directories\n",
    "extract_dir_025 = '../data/fairface_025'\n",
    "extract_dir_125 = '../data/fairface_125'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceec50df-def8-4806-9d33-045603705c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\fairface_025 already exists, skipping extraction.\n",
      "..\\data\\fairface_125 already exists, skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "def extract_zip(zip_path, extract_to):\n",
    "    zip_path = Path(zip_path)\n",
    "    extract_to = Path(extract_to)\n",
    "    \n",
    "    if not extract_to.exists():\n",
    "        print(f\"Extracting {zip_path.name}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Extracted to {extract_to}\")\n",
    "    else:\n",
    "        print(f\"{extract_to} already exists, skipping extraction.\")\n",
    "\n",
    "# Extract both datasets\n",
    "extract_zip(zip_025, extract_dir_025)\n",
    "extract_zip(zip_125, extract_dir_125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c31d8e2-281e-482d-b418-554bb22dfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation labels\n",
    "train_labels = pd.read_csv('../data/fairface_label_train.csv')\n",
    "val_labels = pd.read_csv('../data/fairface_label_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9417950-d011-4a39-a8fe-d5e76923af2d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7effcd0c-6ef6-4048-88bb-2cb0618898dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97698, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>service_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/1.jpg</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/2.jpg</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3.jpg</td>\n",
       "      <td>3-9</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/4.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/5.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file    age  gender        race  service_test\n",
       "0  train/1.jpg  50-59    Male  East Asian          True\n",
       "1  train/2.jpg  30-39  Female      Indian         False\n",
       "2  train/3.jpg    3-9  Female       Black         False\n",
       "3  train/4.jpg  20-29  Female      Indian          True\n",
       "4  train/5.jpg  20-29  Female      Indian          True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine for overall statistics\n",
    "df = pd.concat([train_labels, val_labels], ignore_index=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5667d22-547e-4624-a9a7-a19b4408b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file            0\n",
      "age             0\n",
      "gender          0\n",
      "race            0\n",
      "service_test    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc15609c-1275-464d-8cac-a67d4a410531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97698 entries, 0 to 97697\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   file          97698 non-null  object\n",
      " 1   age           97698 non-null  object\n",
      " 2   gender        97698 non-null  object\n",
      " 3   race          97698 non-null  object\n",
      " 4   service_test  97698 non-null  bool  \n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 3.1+ MB\n",
      "None\n",
      "age: ['50-59' '30-39' '3-9' '20-29' '40-49' '10-19' '60-69' '0-2'\n",
      " 'more than 70']\n",
      "gender: ['Male' 'Female']\n",
      "race: ['East Asian' 'Indian' 'Black' 'White' 'Middle Eastern' 'Latino_Hispanic'\n",
      " 'Southeast Asian']\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "for col in ['age', 'gender', 'race']:\n",
    "    print(f\"{col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a5a38-83df-42b5-80fe-83bc6edecd64",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8178970c-8e3b-42b0-bc71-4d4cfd43857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness(img_path):\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    return np.array(img).mean()\n",
    "\n",
    "df['brightness'] = df['file'].apply(lambda f: brightness(os.path.join('../data/fairface_025', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b733bd1b-201e-4dde-81f5-dce563bba446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast(img_path):\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    return np.array(img).std()\n",
    "\n",
    "df['contrast'] = df['file'].apply(lambda f: contrast(os.path.join('../data/fairface_025', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780b96a9-1668-4a08-922e-eb9bb7127006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image sharpness using Laplacian variance\n",
    "def extract_sharpness(image):\n",
    "    image = image.convert('L')  # Convert to grayscale\n",
    "    image_array = np.array(image)\n",
    "    laplacian = np.abs(np.gradient(np.gradient(image_array)[0])[0]) + np.abs(np.gradient(np.gradient(image_array)[1])[1])\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "df['sharpness'] = df['file'].apply(lambda f: extract_sharpness(Image.open(os.path.join('../data/fairface_025', f))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81d750b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>service_test</th>\n",
       "      <th>brightness</th>\n",
       "      <th>contrast</th>\n",
       "      <th>sharpness</th>\n",
       "      <th>saturation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/1.jpg</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>True</td>\n",
       "      <td>34.598334</td>\n",
       "      <td>14.607521</td>\n",
       "      <td>1.613450</td>\n",
       "      <td>133.303133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/2.jpg</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>False</td>\n",
       "      <td>123.987843</td>\n",
       "      <td>27.774537</td>\n",
       "      <td>2.137719</td>\n",
       "      <td>89.485591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3.jpg</td>\n",
       "      <td>3-9</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>False</td>\n",
       "      <td>144.705138</td>\n",
       "      <td>43.936752</td>\n",
       "      <td>14.794022</td>\n",
       "      <td>77.438776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/4.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>85.136998</td>\n",
       "      <td>71.030456</td>\n",
       "      <td>9.986235</td>\n",
       "      <td>86.136181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/5.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>132.422413</td>\n",
       "      <td>45.385137</td>\n",
       "      <td>10.499576</td>\n",
       "      <td>125.124442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file    age  gender        race  service_test  brightness  \\\n",
       "0  train/1.jpg  50-59    Male  East Asian          True   34.598334   \n",
       "1  train/2.jpg  30-39  Female      Indian         False  123.987843   \n",
       "2  train/3.jpg    3-9  Female       Black         False  144.705138   \n",
       "3  train/4.jpg  20-29  Female      Indian          True   85.136998   \n",
       "4  train/5.jpg  20-29  Female      Indian          True  132.422413   \n",
       "\n",
       "    contrast  sharpness  saturation  \n",
       "0  14.607521   1.613450  133.303133  \n",
       "1  27.774537   2.137719   89.485591  \n",
       "2  43.936752  14.794022   77.438776  \n",
       "3  71.030456   9.986235   86.136181  \n",
       "4  45.385137  10.499576  125.124442  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract saturation\n",
    "def saturation(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_hsv = img.convert('HSV')\n",
    "    saturation_channel = np.array(img_hsv)[:, :, 1]\n",
    "    return saturation_channel.mean()\n",
    "\n",
    "df['saturation'] = df['file'].apply(lambda f: saturation(os.path.join('../data/fairface_025', f)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe316f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>service_test</th>\n",
       "      <th>brightness</th>\n",
       "      <th>contrast</th>\n",
       "      <th>sharpness</th>\n",
       "      <th>saturation</th>\n",
       "      <th>hue_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/1.jpg</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>True</td>\n",
       "      <td>34.598334</td>\n",
       "      <td>14.607521</td>\n",
       "      <td>1.613450</td>\n",
       "      <td>133.303133</td>\n",
       "      <td>5861.253941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/2.jpg</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>False</td>\n",
       "      <td>123.987843</td>\n",
       "      <td>27.774537</td>\n",
       "      <td>2.137719</td>\n",
       "      <td>89.485591</td>\n",
       "      <td>649.939604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3.jpg</td>\n",
       "      <td>3-9</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>False</td>\n",
       "      <td>144.705138</td>\n",
       "      <td>43.936752</td>\n",
       "      <td>14.794022</td>\n",
       "      <td>77.438776</td>\n",
       "      <td>2981.514741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/4.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>85.136998</td>\n",
       "      <td>71.030456</td>\n",
       "      <td>9.986235</td>\n",
       "      <td>86.136181</td>\n",
       "      <td>5501.222295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/5.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>True</td>\n",
       "      <td>132.422413</td>\n",
       "      <td>45.385137</td>\n",
       "      <td>10.499576</td>\n",
       "      <td>125.124442</td>\n",
       "      <td>2881.653201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file    age  gender        race  service_test  brightness  \\\n",
       "0  train/1.jpg  50-59    Male  East Asian          True   34.598334   \n",
       "1  train/2.jpg  30-39  Female      Indian         False  123.987843   \n",
       "2  train/3.jpg    3-9  Female       Black         False  144.705138   \n",
       "3  train/4.jpg  20-29  Female      Indian          True   85.136998   \n",
       "4  train/5.jpg  20-29  Female      Indian          True  132.422413   \n",
       "\n",
       "    contrast  sharpness  saturation  hue_variance  \n",
       "0  14.607521   1.613450  133.303133   5861.253941  \n",
       "1  27.774537   2.137719   89.485591    649.939604  \n",
       "2  43.936752  14.794022   77.438776   2981.514741  \n",
       "3  71.030456   9.986235   86.136181   5501.222295  \n",
       "4  45.385137  10.499576  125.124442   2881.653201  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract hue variance\n",
    "def hue_variance(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_hsv = img.convert('HSV')\n",
    "    hue_channel = np.array(img_hsv)[:, :, 0]\n",
    "    return hue_channel.var()\n",
    "\n",
    "df['hue_variance'] = df['file'].apply(lambda f: hue_variance(os.path.join('../data/fairface_025', f)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daef6427",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m df_landmarks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 29\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_facial_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/fairface_025\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m landmarks:\n\u001b[0;32m     31\u001b[0m         df_landmarks\u001b[38;5;241m.\u001b[39mappend(landmarks)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mextract_facial_landmarks\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m image_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[1;32m---> 12\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmp_face_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceMesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefine_landmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mface_mesh\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mface_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_face_landmarks\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nigel\\AppData\\Local\\r-miniconda\\envs\\py311\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:604\u001b[0m, in \u001b[0;36mSolutionBase.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m    603\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Closes all the input sources and the graph.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nigel\\AppData\\Local\\r-miniconda\\envs\\py311\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:362\u001b[0m, in \u001b[0;36mSolutionBase.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClosing SolutionBase._graph which is already None\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 362\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_stream_type_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# detect faces and extract facial landmarks (eyes, nose, mouth, chin) using mediapipe\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "def extract_facial_landmarks(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5) as face_mesh:\n",
    "        results = face_mesh.process(image_np)\n",
    "        \n",
    "        if not results.multi_face_landmarks:\n",
    "            return None  # No face detected\n",
    "        \n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "        landmark_coords = {}\n",
    "        \n",
    "        for idx, landmark in enumerate(landmarks.landmark):\n",
    "            landmark_coords[f'landmark_{idx}_x'] = landmark.x\n",
    "            landmark_coords[f'landmark_{idx}_y'] = landmark.y\n",
    "        \n",
    "        return landmark_coords\n",
    "    \n",
    "df_landmarks = []\n",
    "for f in df['file']:\n",
    "    landmarks = extract_facial_landmarks(os.path.join('../data/fairface_025', f))\n",
    "    if landmarks:\n",
    "        df_landmarks.append(landmarks)\n",
    "    else:\n",
    "        df_landmarks.append({})  # Append empty dict if no landmarks found\n",
    "df_landmarks_df = pd.DataFrame(df_landmarks)\n",
    "df = pd.concat([df, df_landmarks_df], axis=1)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
